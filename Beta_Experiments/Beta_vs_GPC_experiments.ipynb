{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d13b5d04",
   "metadata": {},
   "source": [
    "# TODOs\n",
    "\n",
    "- Understand the metrics used in the GPC paper and reimplement/copy them\n",
    "- Understand the exact setup for 2D classification in the GPC paper and reimplement/copy it\n",
    "- Look at which data they use and build the pipeline to use it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a363e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gpytorch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.likelihoods import DirichletClassificationLikelihood\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e598620",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "The GPC paper reports three different metrics.\n",
    "- error rate: Inverse of accuracy?\n",
    "- MNLL (mean? negative log likelihood): mean negative log-likelihood of a categorical using probabilities from GP\n",
    "- ECE (expected calibration error): TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da00ae18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    y_pred: predicted probabilities, assumes 2D tensor of shape #samples x #features\n",
    "    y_true: ground truth, assumes a 1D tensor of shape #samples\n",
    "    returns the accuracy of the predicted probabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    max_pred = torch.argmax(y_pred, 1)\n",
    "    acc = torch.mean((max_pred==y_true).float())\n",
    "    return(acc)\n",
    "    \n",
    "def mean_neg_log_likelihood(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    y_pred: predicted probabilities, assumes 2D tensor of shape #samples x #features\n",
    "    y_true: ground truth, assumes a 1D tensor of shape #samples\n",
    "    returns the mean NLL of the predicted probabilities given the true labels\n",
    "    \"\"\"\n",
    "    cat = -torch.distributions.categorical.Categorical(y_pred).log_prob(y_true)\n",
    "    return(cat.mean())\n",
    "\n",
    "def expected_calibration_error(y_pred, y_true):\n",
    "    \"\"\"\n",
    "    y_pred: predicted probabilities, assumes 2D tensor of shape #samples x #features\n",
    "    y_true: ground truth, assumes a 1D tensor of shape #samples\n",
    "    returns the ECE of the predicted probabilities given the true labels\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "def calibration_test(y_pred, y_true, nbins=10):\n",
    "    '''\n",
    "    COPIED FROM DGP PAPER\n",
    "    y_pred: predicted probabilities, assumes 2D tensor of shape #samples x #features\n",
    "    y_true: ground truth, assumes a 1D tensor of shape #samples\n",
    "    Returns ece:  Expected Calibration Error\n",
    "            conf: confindence levels (as many as nbins)\n",
    "            accu: accuracy for a certain confidence level\n",
    "                  We are interested in the plot confidence vs accuracy\n",
    "            bin_sizes: how many points lie within a certain confidence level\n",
    "    '''\n",
    "    edges = torch.linspace(0, 1, nbins+1)\n",
    "    accu = torch.zeros(nbins)\n",
    "    conf = torch.zeros(nbins)\n",
    "    bin_sizes = torch.zeros(nbins)\n",
    "    # Multiclass problems are treated by considering the max\n",
    "    pred = torch.argmax(y_pred, dim=1)\n",
    "    prob = torch.max(y_pred, dim=1)[0]\n",
    "    \n",
    "    #\n",
    "    y_true = y_true.view(-1)\n",
    "    prob = prob.view(-1)\n",
    "    for i in range(nbins):\n",
    "        idx_in_bin = (prob > edges[i]) & (prob <= edges[i+1])\n",
    "        bin_sizes[i] = max(sum(idx_in_bin), 1)\n",
    "        accu[i] = torch.sum(y_true[idx_in_bin] == pred[idx_in_bin]) / bin_sizes[i]\n",
    "        conf[i] = (edges[i+1] + edges[i]) / 2\n",
    "    ece = torch.sum(torch.abs(accu - conf) * bin_sizes) / torch.sum(bin_sizes)\n",
    "    return ece, conf, accu, bin_sizes\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd21ca1c",
   "metadata": {},
   "source": [
    "# Setup for GPC & LB(Beta)+GP\n",
    "\n",
    "create function that makes train-test split\n",
    "\n",
    "create function that gets the right number of inducing points from test data\n",
    "\n",
    "create functions that take in data and return a fitted GP\n",
    "\n",
    "create a function that takes in the fitted GP and returns all the relevant measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "360e7db3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4329.23</td>\n",
       "      <td>4009.23</td>\n",
       "      <td>4289.23</td>\n",
       "      <td>4148.21</td>\n",
       "      <td>4350.26</td>\n",
       "      <td>4586.15</td>\n",
       "      <td>4096.92</td>\n",
       "      <td>4641.03</td>\n",
       "      <td>4222.05</td>\n",
       "      <td>4238.46</td>\n",
       "      <td>4211.28</td>\n",
       "      <td>4280.51</td>\n",
       "      <td>4635.90</td>\n",
       "      <td>4393.85</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4324.62</td>\n",
       "      <td>4004.62</td>\n",
       "      <td>4293.85</td>\n",
       "      <td>4148.72</td>\n",
       "      <td>4342.05</td>\n",
       "      <td>4586.67</td>\n",
       "      <td>4097.44</td>\n",
       "      <td>4638.97</td>\n",
       "      <td>4210.77</td>\n",
       "      <td>4226.67</td>\n",
       "      <td>4207.69</td>\n",
       "      <td>4279.49</td>\n",
       "      <td>4632.82</td>\n",
       "      <td>4384.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4327.69</td>\n",
       "      <td>4006.67</td>\n",
       "      <td>4295.38</td>\n",
       "      <td>4156.41</td>\n",
       "      <td>4336.92</td>\n",
       "      <td>4583.59</td>\n",
       "      <td>4096.92</td>\n",
       "      <td>4630.26</td>\n",
       "      <td>4207.69</td>\n",
       "      <td>4222.05</td>\n",
       "      <td>4206.67</td>\n",
       "      <td>4282.05</td>\n",
       "      <td>4628.72</td>\n",
       "      <td>4389.23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4328.72</td>\n",
       "      <td>4011.79</td>\n",
       "      <td>4296.41</td>\n",
       "      <td>4155.90</td>\n",
       "      <td>4343.59</td>\n",
       "      <td>4582.56</td>\n",
       "      <td>4097.44</td>\n",
       "      <td>4630.77</td>\n",
       "      <td>4217.44</td>\n",
       "      <td>4235.38</td>\n",
       "      <td>4210.77</td>\n",
       "      <td>4287.69</td>\n",
       "      <td>4632.31</td>\n",
       "      <td>4396.41</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4326.15</td>\n",
       "      <td>4011.79</td>\n",
       "      <td>4292.31</td>\n",
       "      <td>4151.28</td>\n",
       "      <td>4347.69</td>\n",
       "      <td>4586.67</td>\n",
       "      <td>4095.90</td>\n",
       "      <td>4627.69</td>\n",
       "      <td>4210.77</td>\n",
       "      <td>4244.10</td>\n",
       "      <td>4212.82</td>\n",
       "      <td>4288.21</td>\n",
       "      <td>4632.82</td>\n",
       "      <td>4398.46</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0        1        2        3        4        5        6        7   \\\n",
       "0  4329.23  4009.23  4289.23  4148.21  4350.26  4586.15  4096.92  4641.03   \n",
       "1  4324.62  4004.62  4293.85  4148.72  4342.05  4586.67  4097.44  4638.97   \n",
       "2  4327.69  4006.67  4295.38  4156.41  4336.92  4583.59  4096.92  4630.26   \n",
       "3  4328.72  4011.79  4296.41  4155.90  4343.59  4582.56  4097.44  4630.77   \n",
       "4  4326.15  4011.79  4292.31  4151.28  4347.69  4586.67  4095.90  4627.69   \n",
       "\n",
       "        8        9        10       11       12       13  14  \n",
       "0  4222.05  4238.46  4211.28  4280.51  4635.90  4393.85   0  \n",
       "1  4210.77  4226.67  4207.69  4279.49  4632.82  4384.10   0  \n",
       "2  4207.69  4222.05  4206.67  4282.05  4628.72  4389.23   0  \n",
       "3  4217.44  4235.38  4210.77  4287.69  4632.31  4396.41   0  \n",
       "4  4210.77  4244.10  4212.82  4288.21  4632.82  4398.46   0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### load an example dataset\n",
    "data_files = \"/home/marius/Desktop/Laplace_Matching_for_GLMs/Beta_Experiments/data/\"\n",
    "\n",
    "filename = \"EEG_Eye_State.csv\"\n",
    "EEG_df = pd.read_csv(data_files + filename, header=None)\n",
    "EEG_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2172c068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10980, 14)\n",
      "(4000, 14)\n",
      "(10980,)\n",
      "(4000,)\n",
      "tensor([1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "# make test-train split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_EEG = EEG_df.values[:,:-1]\n",
    "Y_EEG = EEG_df.values[:,-1]\n",
    "\n",
    "# The original paper chose 10980 training points and 4000 test points\n",
    "test_size_EEG = 4000/(10980+4000)\n",
    "X_EEG_train, X_EEG_test, y_EEG_train, y_EEG_test = train_test_split(X_EEG, Y_EEG, test_size=test_size_EEG, random_state=42)\n",
    "print(np.shape(X_EEG_train))\n",
    "print(np.shape(X_EEG_test))\n",
    "print(np.shape(y_EEG_train))\n",
    "print(np.shape(y_EEG_test))\n",
    "X_EEG_train, X_EEG_test, y_EEG_train, y_EEG_test = torch.tensor(X_EEG_train).float(), torch.tensor(X_EEG_test).float(), \\\n",
    "                                                   torch.tensor(y_EEG_train).long(), torch.tensor(y_EEG_test).long()\n",
    "print(y_EEG_test[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd82609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create k-means cluster for the inducing points\n",
    "\n",
    "def k_means_inducing_points(train_x, train_y, num_inducing_points):\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff8550ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9298/269273252.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_EEG_train_induced = torch.tensor(X_EEG_train[inducing_idxs]).float()\n",
      "/tmp/ipykernel_9298/269273252.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_EEG_train_induced = torch.tensor(y_EEG_train[inducing_idxs]).long()\n"
     ]
    }
   ],
   "source": [
    "# ignore for now and just use random subsets\n",
    "\n",
    "num_inducing_points = 1000\n",
    "inducing_idxs = np.random.choice(10980, num_inducing_points, replace=False)\n",
    "X_EEG_train_induced = torch.tensor(X_EEG_train[inducing_idxs]).float()\n",
    "y_EEG_train_induced = torch.tensor(y_EEG_train[inducing_idxs]).long()\n",
    "\n",
    "# TODO use k-means clustering and conjugacy to create better inducing points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb7a7e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DirichletGPModel(\n",
      "  (likelihood): DirichletClassificationLikelihood(\n",
      "    (noise_covar): FixedGaussianNoise()\n",
      "    (second_noise_covar): HomoskedasticNoise(\n",
      "      (raw_noise_constraint): GreaterThan(1.000E-04)\n",
      "    )\n",
      "  )\n",
      "  (mean_module): ConstantMean()\n",
      "  (covar_module): ScaleKernel(\n",
      "    (base_kernel): RBFKernel(\n",
      "      (raw_lengthscale_constraint): Positive()\n",
      "    )\n",
      "    (raw_outputscale_constraint): Positive()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# create a generic function for the GPC\n",
    "# returns dirichlet gaussian process classification model and likelihood\n",
    "\n",
    "def create_DGP_model(train_x, train_y, learn_additional_noise=True):\n",
    "\n",
    "    class DirichletGPModel(ExactGP):\n",
    "        def __init__(self, train_x, train_y, likelihood, num_classes):\n",
    "            super(DirichletGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "            self.mean_module = ConstantMean(batch_shape=torch.Size((num_classes,)))\n",
    "            self.covar_module = ScaleKernel(\n",
    "                RBFKernel(batch_shape=torch.Size((num_classes,))),\n",
    "                batch_shape=torch.Size((num_classes,)),\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            mean_x = self.mean_module(x)\n",
    "            covar_x = self.covar_module(x)\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "    # initialize likelihood and model\n",
    "    # we let the DirichletClassificationLikelihood compute the targets for us\n",
    "    likelihood = DirichletClassificationLikelihood(train_y, learn_additional_noise=learn_additional_noise)\n",
    "    model = DirichletGPModel(train_x, likelihood.transformed_targets, likelihood, num_classes=likelihood.num_classes)\n",
    "    return(model, likelihood)\n",
    "\n",
    "DGP_model, DGP_likelihood = create_DGP_model(X_EEG_train_induced, y_EEG_train_induced)\n",
    "print(DGP_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54afd590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/500 - Loss: 7.102   lengthscale: 0.693   noise: 2.652\n",
      "Iter 51/500 - Loss: 5.182   lengthscale: 0.693   noise: 2.652\n",
      "Iter 101/500 - Loss: 5.170   lengthscale: 0.693   noise: 2.652\n",
      "Iter 151/500 - Loss: 5.172   lengthscale: 0.693   noise: 2.652\n",
      "Iter 201/500 - Loss: 5.174   lengthscale: 0.693   noise: 2.652\n",
      "Iter 251/500 - Loss: 5.173   lengthscale: 0.693   noise: 2.652\n",
      "Iter 301/500 - Loss: 5.172   lengthscale: 0.693   noise: 2.652\n",
      "Iter 351/500 - Loss: 5.174   lengthscale: 0.693   noise: 2.652\n",
      "Iter 401/500 - Loss: 5.171   lengthscale: 0.693   noise: 2.652\n",
      "Iter 451/500 - Loss: 5.169   lengthscale: 0.693   noise: 2.652\n"
     ]
    }
   ],
   "source": [
    "def train_DGP_model(train_x, model, likelihood, num_iter=500, lr=0.1, report_iter=50):\n",
    "    \n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    for i in range(num_iter):\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(train_x)\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(output, likelihood.transformed_targets).sum()\n",
    "        loss.backward()\n",
    "        #print(model.likelihood)\n",
    "        if i % report_iter == 0:\n",
    "            print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "                i + 1, num_iter, loss.item(),\n",
    "                model.covar_module.base_kernel.lengthscale.mean().item(),\n",
    "                model.likelihood.noise_covar.noise.mean().item()\n",
    "            ))\n",
    "        optimizer.step()\n",
    "        \n",
    "    return(model, likelihood)\n",
    "\n",
    "DGP_model, DGP_likelihood = train_DGP_model(X_EEG_train_induced,\n",
    "                                            DGP_model, DGP_likelihood, num_iter=500, lr=0.1, report_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3d978e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMGPModel(\n",
      "  (likelihood): FixedNoiseGaussianLikelihood(\n",
      "    (noise_covar): FixedGaussianNoise()\n",
      "  )\n",
      "  (mean_module): ConstantMean()\n",
      "  (covar_module): ScaleKernel(\n",
      "    (base_kernel): RBFKernel(\n",
      "      (raw_lengthscale_constraint): Positive()\n",
      "    )\n",
      "    (raw_outputscale_constraint): Positive()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# create a generic function for the LM+Beta process\n",
    "\n",
    "def LB_beta(alpha, beta):\n",
    "    \n",
    "    mu = np.log(alpha/beta)\n",
    "    var = (alpha+beta)/(alpha*beta)\n",
    "    return(mu, var)\n",
    "\n",
    "def transform_y_beta_LM(train_y, eps_alpha=0.1, eps_beta=0.1):\n",
    "\n",
    "    train_alphas = torch.ones_like(train_y) * eps_alpha\n",
    "    train_alphas[train_y > 0.5] += 1\n",
    "    train_betas = torch.ones_like(train_y) * eps_beta\n",
    "    train_betas[train_y < 0.5] += 1\n",
    "\n",
    "    train_mu_LB, train_var_LB = LB_beta(train_alphas, train_betas)\n",
    "    return(train_mu_LB, train_var_LB)\n",
    "\n",
    "def create_LMGP_model(train_x, train_y_mu, train_y_var, learn_additional_noise=False):\n",
    "    \n",
    "    # We will use the simplest form of GP model, exact inference\n",
    "    class LMGPModel(ExactGP):\n",
    "        def __init__(self, train_x, train_y, likelihood):\n",
    "            super(LMGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "            self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "        def forward(self, x):\n",
    "            mean_x = self.mean_module(x)\n",
    "            covar_x = self.covar_module(x)\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "    # initialize likelihood and model\n",
    "    likelihood = gpytorch.likelihoods.FixedNoiseGaussianLikelihood(noise=torch.sqrt(train_y_var), \n",
    "                                                                   learn_additional_noise=learn_additional_noise)\n",
    "    model = LMGPModel(train_x, train_y_mu, likelihood)\n",
    "    return(model, likelihood)\n",
    "\n",
    "y_EEG_train_induced_mu, y_EEG_train_induced_var = transform_y_beta_LM(y_EEG_train_induced)\n",
    "LMGP_model, LMGP_likelihood = create_LMGP_model(X_EEG_train_induced, y_EEG_train_induced_mu,\n",
    "                                                y_EEG_train_induced_var, learn_additional_noise=False)\n",
    "print(LMGP_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af72b7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/500 - Loss: 2.305   lengthscale: 0.693\n",
      "Iter 51/500 - Loss: 2.281   lengthscale: 0.693\n",
      "Iter 101/500 - Loss: 2.281   lengthscale: 0.693\n",
      "Iter 151/500 - Loss: 2.281   lengthscale: 0.693\n",
      "Iter 201/500 - Loss: 2.281   lengthscale: 0.693\n",
      "Iter 251/500 - Loss: 2.281   lengthscale: 0.693\n",
      "Iter 301/500 - Loss: 2.281   lengthscale: 0.693\n",
      "Iter 351/500 - Loss: 2.281   lengthscale: 0.693\n",
      "Iter 401/500 - Loss: 2.281   lengthscale: 0.693\n",
      "Iter 451/500 - Loss: 2.281   lengthscale: 0.693\n"
     ]
    }
   ],
   "source": [
    "# generic function to train LM+Beta GP\n",
    "def train_LMGP_model(train_x, train_y_mu, model, likelihood, num_iter=500, lr=0.1, report_iter=50):\n",
    "    \n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    for i in range(num_iter):\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(train_x)\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(output, train_y_mu)\n",
    "        loss.backward()\n",
    "        #print(model.likelihood)\n",
    "        if i % report_iter == 0:\n",
    "            print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f' % (\n",
    "                i + 1, num_iter, loss.item(),\n",
    "                model.covar_module.base_kernel.lengthscale.mean().item()\n",
    "            ))\n",
    "        optimizer.step()\n",
    "        \n",
    "    return(model, likelihood)\n",
    "\n",
    "LMGP_model, LMGP_likelihood = train_LMGP_model(X_EEG_train_induced, y_EEG_train_induced_mu,\n",
    "                                            LMGP_model, LMGP_likelihood, num_iter=500, lr=0.1, report_iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b036e97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4000, 2]) torch.Size([4000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.5305), tensor(0.8109), tensor(0.2201))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate DGP\n",
    "def evaluate_DGP(model, likelihood, test_x, test_y, num_samples=1000):\n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    with gpytorch.settings.fast_pred_var(), torch.no_grad():\n",
    "        test_dist = model(test_x)\n",
    "\n",
    "    pred_samples = test_dist.sample(torch.Size((num_samples,)))\n",
    "    pred_samples_exp = pred_samples.exp()\n",
    "    #probs\n",
    "    pred_samples_norm = (pred_samples_exp / pred_samples_exp.sum(-2, keepdim=True))\n",
    "    pred_probs = pred_samples_norm.mean(0)\n",
    "    pred_probs = torch.swapaxes(pred_probs, 0, 1)\n",
    "    \n",
    "    print(pred_probs.size(), test_y.size())\n",
    "    #print(pred_probs[:50], test_y[:50])\n",
    "    \n",
    "    # evaluate\n",
    "    acc = get_accuracy(pred_probs, test_y)\n",
    "    mnll = mean_neg_log_likelihood(pred_probs, test_y)\n",
    "    ece, conf, accu, bin_sizes = calibration_test(pred_probs, test_y, nbins=10)\n",
    "    \n",
    "    return(acc, mnll, ece)\n",
    "    \n",
    "evaluate_DGP(DGP_model, DGP_likelihood, X_EEG_test, y_EEG_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "296c5ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4000, 2])\n",
      "torch.Size([4000, 2]) torch.Size([4000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.5305), tensor(0.6994), tensor(0.0206))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate LG+Beta GP\n",
    "def logistic(x):\n",
    "    return(1 / (1 + np.exp(-x)))\n",
    "\n",
    "def evaluate_LMGP(model, likelihood, test_x, test_y, num_samples=1000):\n",
    "    \n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    # Make predictions by feeding model through likelihood\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        observed_pred = likelihood(model(test_x))\n",
    "\n",
    "    pred_mean = observed_pred.mean\n",
    "    pred_probs = logistic(pred_mean)\n",
    "    # make 2D\n",
    "    pred_probs = torch.cat([1-pred_probs.view(-1,1), pred_probs.view(-1,1)], dim=1)\n",
    "    print(pred_probs.size())\n",
    "    \n",
    "    print(pred_probs.size(), test_y.size())\n",
    "    #print(pred_probs[:50], test_y[:50])\n",
    "    \n",
    "    # evaluate\n",
    "    acc = get_accuracy(pred_probs, test_y)\n",
    "    mnll = mean_neg_log_likelihood(pred_probs, test_y)\n",
    "    ece, conf, accu, bin_sizes = calibration_test(pred_probs, test_y, nbins=10)\n",
    "    \n",
    "    return(acc, mnll, ece)\n",
    "    \n",
    "evaluate_LMGP(LMGP_model, LMGP_likelihood, X_EEG_test, y_EEG_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dff1e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b01685",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b6074c51",
   "metadata": {},
   "source": [
    "# find all datasets and run all measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea24f73f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
