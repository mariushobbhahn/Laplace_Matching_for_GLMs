{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8667e3d7",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "TODO: description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11f937ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29b8dca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create synthetic dataset as shown in original GPC paper\n",
    "## https://github.com/dmilios/dirichletGPC/blob/master/run_dirichletGP_example.py\n",
    "## ===============================\n",
    "\n",
    "\n",
    "N = 20  # training data\n",
    "np.random.seed(1235)\n",
    "\n",
    "xmax = 15\n",
    "X = np.random.rand(N,1) * xmax\n",
    "Xtest = np.linspace(0, xmax*1.5, 200).reshape(-1, 1)\n",
    "Z = X.copy()\n",
    "\n",
    "y = np.cos(X.flatten()) / 2 + 0.5\n",
    "y = np.random.rand(y.size) > y\n",
    "y = y.astype(int)\n",
    "if np.sum(y==1) == 0:\n",
    "    y[0] = 1\n",
    "elif np.sum(y==0) == 0:\n",
    "    y[0] = 0\n",
    "\n",
    "# one-hot vector encoding\n",
    "Y01 = np.zeros((y.size, 2))\n",
    "Y01[:,0], Y01[:,1] = 1-y, y\n",
    "\n",
    "\n",
    "### latent log process\n",
    "a_eps=0.1\n",
    "s2_tilde = np.log(1.0/(Y01+a_eps) + 1)\n",
    "Y_tilde = np.log(Y01+a_eps) - 0.5 * s2_tilde\n",
    "\n",
    "\n",
    "# For each y, we have two possibilities: 0+alpha and 1+alpha\n",
    "# Changing alpha (the scale of Gamma) changes the distance\n",
    "# between different class instances.\n",
    "# Changing beta (the rate of Gamma) changes the position \n",
    "# (i.e. log(alpha)-log(beta)-s2_tilde/2 ) but NOT the distance.\n",
    "# Thus, we can simply move y for all classes to our convenience (ie zero mean)\n",
    "\n",
    "# 1st term: guarantees that the prior class probabilities are correct\n",
    "# 2nd term: just makes the latent processes zero-mean\n",
    "#ymean = np.log(Y01.mean(0)) + np.mean(Y_tilde-np.log(Y01.mean(0)))\n",
    "#Y_tilde = Y_tilde - ymean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb341e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## plot samples\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 4))\n",
    "\n",
    "ax.scatter(X, y, s=50, color=\"k\")\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddbd7343",
   "metadata": {},
   "outputs": [],
   "source": [
    "## build GPC model using GPyTorch\n",
    "\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.likelihoods import DirichletClassificationLikelihood\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "\n",
    "num_classes = 2\n",
    "train_x = torch.tensor(X).float()\n",
    "train_y = torch.tensor(y)\n",
    "test_x = torch.tensor(np.linspace(0-5, xmax+5, 100)).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9135a1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the simplest form of GP model, exact inference\n",
    "class DirichletGPModel(ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, num_classes):\n",
    "        super(DirichletGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = ConstantMean(batch_shape=torch.Size((num_classes,)))\n",
    "        self.covar_module = ScaleKernel(\n",
    "            RBFKernel(batch_shape=torch.Size((num_classes,))),\n",
    "            batch_shape=torch.Size((num_classes,)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# initialize likelihood and model\n",
    "# we let the DirichletClassificationLikelihood compute the targets for us\n",
    "likelihood = DirichletClassificationLikelihood(train_y, learn_additional_noise=True)\n",
    "model = DirichletGPModel(train_x, likelihood.transformed_targets, likelihood, num_classes=likelihood.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4d8c5f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/50 - Loss: 6.572   lengthscale: 0.693   noise: 0.693\n",
      "Iter 6/50 - Loss: 5.943   lengthscale: 0.968   noise: 0.973\n",
      "Iter 11/50 - Loss: 5.597   lengthscale: 1.230   noise: 1.303\n",
      "Iter 16/50 - Loss: 5.405   lengthscale: 1.329   noise: 1.670\n",
      "Iter 21/50 - Loss: 5.272   lengthscale: 1.249   noise: 2.047\n",
      "Iter 26/50 - Loss: 5.181   lengthscale: 1.092   noise: 2.394\n",
      "Iter 31/50 - Loss: 5.128   lengthscale: 0.946   noise: 2.681\n",
      "Iter 36/50 - Loss: 5.098   lengthscale: 0.848   noise: 2.893\n",
      "Iter 41/50 - Loss: 5.080   lengthscale: 0.802   noise: 3.033\n",
      "Iter 46/50 - Loss: 5.068   lengthscale: 0.796   noise: 3.111\n"
     ]
    }
   ],
   "source": [
    "# this is for running the notebook in our testing framework\n",
    "training_iter = 50\n",
    "\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, likelihood.transformed_targets).sum()\n",
    "    loss.backward()\n",
    "    if i % 5 == 0:\n",
    "        print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "            i + 1, training_iter, loss.item(),\n",
    "            model.covar_module.base_kernel.lengthscale.mean().item(),\n",
    "            model.likelihood.second_noise_covar.noise.mean().item()\n",
    "        ))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b130ea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with gpytorch.settings.fast_pred_var(), torch.no_grad():\n",
    "    test_dist = model(test_x)\n",
    "    pred_means = test_dist.loc\n",
    "    \n",
    "pred_samples = test_dist.sample(torch.Size((256,)))\n",
    "pred_samples_exp = pred_samples.exp()\n",
    "pred_samples_mean = pred_samples.mean(0)\n",
    "pred_samples_lb = np.percentile(pred_samples[:,1,:].numpy(), 2.5, axis=0)\n",
    "pred_samples_ub = np.percentile(pred_samples[:,1,:].numpy(), 97.5, axis=0)\n",
    "\n",
    "#probs\n",
    "pred_samples_norm = (pred_samples_exp / pred_samples_exp.sum(-2, keepdim=True))\n",
    "probabilities = pred_samples_norm.mean(0)\n",
    "pred_samples_prob_lb = np.percentile(pred_samples_norm[:,1,:].numpy(), 2.5, axis=0)\n",
    "pred_samples_prob_ub = np.percentile(pred_samples_norm[:,1,:].numpy(), 97.5, axis=0)\n",
    "#print(pred_samples_lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3575c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "## plot samples\n",
    "\n",
    "# fix y_tilde to have the right height (it doesn't matter for the probabilities)\n",
    "y_tilde_plot = Y_tilde[:,1] - (Y_tilde[:,1].max() - pred_samples_mean[1].max()).item()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 3))\n",
    "\n",
    "ax[0].scatter(X, y_tilde_plot, s=50, color=\"k\")\n",
    "ax[0].plot(test_x, pred_samples_mean[1], color=\"orange\")\n",
    "ax[0].fill_between(test_x, pred_samples_ub, pred_samples_lb, alpha=0.2)\n",
    "\n",
    "\n",
    "ax[1].scatter(X, y, s=50, color=\"k\")\n",
    "ax[1].plot(test_x, probabilities[1].numpy(), color=\"orange\")\n",
    "#ax.scatter(test_x, pred_samples_norm[:,0,:], alpha=0.1)\n",
    "ax[1].fill_between(test_x, pred_samples_prob_ub, pred_samples_prob_lb, alpha=0.2)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb9ceae",
   "metadata": {},
   "source": [
    "# Now use LM for Beta for the same task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f486f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define LM for Beta and apply to datapoints\n",
    "\n",
    "def LB_beta(alpha, beta):\n",
    "    \n",
    "    mu = np.log(alpha/beta)\n",
    "    var = (alpha+beta)/(alpha*beta)\n",
    "    return(mu, var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb8cbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_alpha = 0.1\n",
    "eps_beta = 0.1\n",
    "\n",
    "train_alphas = torch.ones_like(train_y) * eps_alpha\n",
    "train_alphas[train_y > 0.5] += 1\n",
    "train_betas = torch.ones_like(train_y) * eps_beta\n",
    "train_betas[train_y < 0.5] += 1\n",
    "\n",
    "train_mu_LB, train_var_LB = LB_beta(train_alphas, train_betas)\n",
    "print(train_mu_LB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd593fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create GP for the now Gaussian datapoints\n",
    "\n",
    "# We will use the simplest form of GP model, exact inference\n",
    "class LBGPModel(ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(LBGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# initialize likelihood and model\n",
    "#likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_covar=torch.sqrt(train_var_LB))#, learn_additional_noise=False)\n",
    "likelihood = gpytorch.likelihoods.FixedNoiseGaussianLikelihood(noise=torch.sqrt(train_var_LB))#, learn_additional_noise=False)\n",
    "model = LBGPModel(train_x, train_mu_LB, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753b0bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "\n",
    "# this is for running the notebook in our testing framework\n",
    "training_iter = 50\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_x.view(-1,1))\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_mu_LB)\n",
    "    loss.backward()\n",
    "    #print(model.covar_module.base_kernel.kernels[0])\n",
    "    \"\"\"\n",
    "    if i % 5 == 0:\n",
    "        print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "            i + 1, training_iter, loss.item(),\n",
    "            model.covar_module.base_kernel.lengthscale.item(),\n",
    "            model.likelihood.noise.item()\n",
    "        ))\n",
    "    \"\"\"\n",
    "    if i % 5 == 0:\n",
    "        print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f'% (\n",
    "            i + 1, training_iter, loss.item(),\n",
    "            model.covar_module.base_kernel.lengthscale.item()\n",
    "        ))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612f30f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Make predictions by feeding model through likelihood\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    observed_pred = likelihood(model(test_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f69b2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x):\n",
    "    return(1 / (1 + np.exp(-x)))\n",
    "\n",
    "pred_mean = observed_pred.mean.numpy()\n",
    "pred_mean_logistic = logistic(pred_mean)\n",
    "lb_pred, ub_pred = observed_pred.confidence_region()\n",
    "lb_pred_logistic, ub_pred_logistic = logistic(lb_pred), logistic(ub_pred)\n",
    "\n",
    "#print(lb_pred_logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2ccf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update plot to contain GPC and LM(Beta)+GP\n",
    "\n",
    "## plot samples\n",
    "\n",
    "# fix y_tilde to have the right height (it doesn't matter for the probabilities)\n",
    "y_tilde_plot = Y_tilde[:,1] - (Y_tilde[:,1].max() - pred_samples_mean[1].max()).item()\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(15, 7))\n",
    "\n",
    "ax[0][0].scatter(X, y_tilde_plot, s=50, color=\"k\")\n",
    "ax[0][0].plot(test_x, pred_samples_mean[1], color=\"orange\")\n",
    "ax[0][0].fill_between(test_x, pred_samples_ub, pred_samples_lb, alpha=0.2)\n",
    "\n",
    "ax[0][1].scatter(X, y, s=50, color=\"k\")\n",
    "ax[0][1].plot(test_x, probabilities[1].numpy(), color=\"orange\")\n",
    "ax[0][1].fill_between(test_x, pred_samples_prob_ub, pred_samples_prob_lb, alpha=0.2)\n",
    "\n",
    "\n",
    "# LB beta\n",
    "ax[1][0].scatter(X, train_mu_LB, s=50, color=\"k\")\n",
    "ax[1][0].plot(test_x, pred_mean, color=\"firebrick\")\n",
    "ax[1][0].fill_between(test_x, lb_pred, ub_pred, alpha=0.2, color=\"cornflowerblue\")\n",
    "\n",
    "ax[1][1].scatter(X, y, s=50, color=\"k\")\n",
    "ax[1][1].plot(test_x, pred_mean_logistic, color=\"firebrick\")\n",
    "ax[1][1].fill_between(test_x, lb_pred_logistic, ub_pred_logistic, alpha=0.2, color=\"cornflowerblue\")\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843064fc",
   "metadata": {},
   "source": [
    "# Using conjugate property to summarize inputs - inducing points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed7f23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create synthetic dataset as shown in original GPC paper\n",
    "## https://github.com/dmilios/dirichletGPC/blob/master/run_dirichletGP_example.py\n",
    "## ===============================\n",
    "\n",
    "\n",
    "N = 200  # training data\n",
    "np.random.seed(1235)\n",
    "\n",
    "xmax = 15\n",
    "X_ = np.random.rand(N,1) * xmax\n",
    "Xtest_ = np.linspace(0, xmax*1.5, 200).reshape(-1, 1)\n",
    "Z_ = X_.copy()\n",
    "\n",
    "y_ = np.cos(X_.flatten()) / 2 + 0.5\n",
    "y_ = np.random.rand(y_.size) > y_\n",
    "y_ = y_.astype(int)\n",
    "if np.sum(y_==1) == 0:\n",
    "    y_[0] = 1\n",
    "elif np.sum(y_==0) == 0:\n",
    "    y_[0] = 0\n",
    "    \n",
    "## plot samples\n",
    "fig, ax = plt.subplots(1, 1, figsize=(7, 4))\n",
    "\n",
    "ax.scatter(X_, y_, s=50, color=\"k\")\n",
    "for i in range(16):\n",
    "    ax.axvline(x=i, linestyle=\"--\", color=\"purple\")\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3f0137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pseudo points with conjugate property of the beta\n",
    "\n",
    "eps_alpha = 0.1\n",
    "eps_beta = 0.1\n",
    "X_ = torch.tensor(X_).view(-1)\n",
    "\n",
    "x_collected = torch.arange(15) + 0.5\n",
    "train_alphas_collected = torch.ones_like(x_collected) * eps_alpha\n",
    "train_betas_collected = torch.ones_like(x_collected) * eps_beta\n",
    "\n",
    "\n",
    "for i in range(15):\n",
    "    y_selected = torch.tensor(y_[torch.logical_and(X_ > i, X_ < i+1)])\n",
    "    ones = torch.sum(y_selected)\n",
    "    zeros = len(y_selected) - ones\n",
    "    train_alphas_collected[i] += ones\n",
    "    train_betas_collected[i] += zeros\n",
    "    \n",
    "print(train_alphas_collected)\n",
    "print(train_betas_collected)\n",
    "\n",
    "\n",
    "train_mu_LB_collected, train_var_LB_collected = LB_beta(train_alphas_collected, train_betas_collected)\n",
    "print(train_mu_LB_collected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98f7a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create GP for the now Gaussian datapoints\n",
    "\n",
    "# We will use the simplest form of GP model, exact inference\n",
    "class LBGPModel2(ExactGP):\n",
    "    def __init__(self, c_collected, train_y, likelihood):\n",
    "        super(LBGPModel2, self).__init__(x_collected, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# initialize likelihood and model\n",
    "#likelihood = gpytorch.likelihoods.GaussianLikelihood(noise_covar=torch.sqrt(train_var_LB))\n",
    "likelihood = gpytorch.likelihoods.FixedNoiseGaussianLikelihood(noise=torch.sqrt(train_var_LB_collected))\n",
    "model = LBGPModel2(x_collected, train_mu_LB_collected, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a979087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "\n",
    "# this is for running the notebook in our testing framework\n",
    "training_iter = 50\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(x_collected.view(-1,1))\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_mu_LB_collected)\n",
    "    loss.backward()\n",
    "    #print(model.covar_module.base_kernel.kernels[0])\n",
    "    \"\"\"\n",
    "    if i % 5 == 0:\n",
    "        print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "            i + 1, training_iter, loss.item(),\n",
    "            model.covar_module.base_kernel.lengthscale.item(),\n",
    "            model.likelihood.noise.item()\n",
    "        ))\n",
    "    \"\"\"\n",
    "    if i % 5 == 0:\n",
    "        print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f'% (\n",
    "            i + 1, training_iter, loss.item(),\n",
    "            model.covar_module.base_kernel.lengthscale.item()\n",
    "        ))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a0322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Make predictions by feeding model through likelihood\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    test_dist = model(test_x)\n",
    "    observed_pred_collected = likelihood(test_dist)\n",
    "    \n",
    "pred_samples = test_dist.sample(torch.Size((10000,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be32614",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mean_collected = observed_pred_collected.mean.numpy()\n",
    "pred_mean_collected_logistic = logistic(pred_mean_collected)\n",
    "lb_pred_collected, ub_pred_collected = observed_pred_collected.confidence_region()\n",
    "lb_pred_collected_logistic, ub_pred_collected_logistic = logistic(lb_pred_collected), logistic(ub_pred_collected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03fba68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update plot to contain GPC and LM(Beta)+GP\n",
    "\n",
    "## plot samples\n",
    "\n",
    "# fix y_tilde to have the right height (it doesn't matter for the probabilities)\n",
    "y_tilde_plot = Y_tilde[:,1] - (Y_tilde[:,1].max() - pred_samples_mean[1].max()).item()\n",
    "\n",
    "fig, ax = plt.subplots(3, 2, figsize=(15, 10))\n",
    "\n",
    "ax[0][0].scatter(X, y_tilde_plot, s=50, color=\"k\")\n",
    "ax[0][0].plot(test_x, pred_samples_mean[1], color=\"orange\")\n",
    "ax[0][0].fill_between(test_x, pred_samples_ub, pred_samples_lb, alpha=0.2)\n",
    "\n",
    "ax[0][1].scatter(X, y, s=50, color=\"k\")\n",
    "ax[0][1].plot(test_x, probabilities[1].numpy(), color=\"orange\")\n",
    "ax[0][1].fill_between(test_x, pred_samples_prob_ub, pred_samples_prob_lb, alpha=0.2)\n",
    "\n",
    "\n",
    "# LB beta\n",
    "ax[1][0].scatter(X, train_mu_LB, s=50, color=\"k\")\n",
    "ax[1][0].plot(test_x, pred_mean, color=\"firebrick\")\n",
    "ax[1][0].fill_between(test_x, lb_pred, ub_pred, alpha=0.2, color=\"cornflowerblue\")\n",
    "\n",
    "ax[1][1].scatter(X, y, s=50, color=\"k\")\n",
    "ax[1][1].plot(test_x, pred_mean_logistic, color=\"firebrick\")\n",
    "ax[1][1].fill_between(test_x, lb_pred_logistic, ub_pred_logistic, alpha=0.2, color=\"cornflowerblue\")\n",
    "\n",
    "\n",
    "# LB beta with many more points + conjugate inducing idea\n",
    "for i in range(16):\n",
    "    ax[2][0].axvline(x=i, linestyle=\"--\", color=\"navy\")\n",
    "ax[2][0].scatter(x_collected, train_mu_LB_collected, s=100, color=\"green\")\n",
    "ax[2][0].plot(test_x, pred_mean_collected, color=\"firebrick\")\n",
    "ax[2][0].fill_between(test_x, lb_pred_collected, ub_pred_collected, alpha=0.2, color=\"cornflowerblue\")\n",
    "\n",
    "\n",
    "for i in range(16):\n",
    "    ax[2][1].axvline(x=i, linestyle=\"--\", color=\"navy\")\n",
    "ax[2][1].scatter(X_, y_, s=50, color=\"k\")\n",
    "ax[2][1].scatter(x_collected, logistic(train_mu_LB_collected), s=100, color=\"forestgreen\")\n",
    "ax[2][1].plot(test_x, pred_mean_collected_logistic, color=\"firebrick\")\n",
    "ax[2][1].fill_between(test_x, lb_pred_collected_logistic, ub_pred_collected_logistic, alpha=0.2, color=\"cornflowerblue\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bea5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose samples in block between 1 and 2\n",
    "logistic(pred_samples).size()\n",
    "\n",
    "# value -5 is at pos 0 in array, 0 is at 20, 1 is at 24, 2 is at 28 and 1.5 is at 26\n",
    "marginal_GP_samples_24 = logistic(pred_samples[:,24]).view(-1).numpy()\n",
    "marginal_GP_samples_25 = logistic(pred_samples[:,25]).view(-1).numpy()\n",
    "marginal_GP_samples_26 = logistic(pred_samples[:,26]).view(-1).numpy()\n",
    "marginal_GP_samples_27 = logistic(pred_samples[:,27]).view(-1).numpy()\n",
    "#print(marginal_GP_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcf6968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot one of the marginals for the Beta distributions\n",
    "\n",
    "a1 = train_alphas_collected[1]\n",
    "b1 = train_betas_collected[1]\n",
    "print(a1, b1)\n",
    "\n",
    "x_01 = np.linspace(0, 1, 1001)\n",
    "eps1 = np.random.random(9)\n",
    "eps1 = (eps1 - eps1.mean())/10 + 0.03\n",
    "print(eps1)\n",
    "\n",
    "eps2 = np.random.random(5)\n",
    "eps2 = (eps2 - eps2.mean())/10 + 0.97\n",
    "print(eps2)\n",
    "\n",
    "from scipy.stats import beta\n",
    "\n",
    "beta_pdf = beta.pdf(x_01, a1, b1)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10, 5), dpi=200)\n",
    "ax.plot(x_01, beta_pdf, label=\"marginal Beta between 1 and 2\", linewidth=3)\n",
    "\n",
    "for i in range(len(eps1)):\n",
    "    ax.axvline(eps1[i], 0, 0.3, color=\"k\")\n",
    "    \n",
    "for i in range(len(eps2)):\n",
    "    ax.axvline(eps2[i], 0, 0.3, color=\"k\")\n",
    "    \n",
    "ax.axvline(eps1[0], 0, 0.3, color=\"k\", label=\"data points between 1 and 2\")\n",
    "ax.hist(marginal_GP_samples_24, density=True, bins=50, color=\"orange\", alpha=0.3, label=\"marginal GP prediction at 1.0\")\n",
    "ax.hist(marginal_GP_samples_25, density=True, bins=50, color=\"green\", alpha=0.3, label=\"marginal GP prediction at 1.25\")\n",
    "ax.hist(marginal_GP_samples_26, density=True, bins=50, color=\"red\", alpha=0.3, label=\"marginal GP prediction at 1.5\")\n",
    "#ax.hist(marginal_GP_samples_27, density=True, bins=50, color=\"cyan\", alpha=0.3, label=\"marginal GP prediction at 1.75\")\n",
    "    \n",
    "ax.set_yticks([])\n",
    "ax.legend()\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61c89eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12226571",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b3a500",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
